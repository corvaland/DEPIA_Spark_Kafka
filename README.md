# DEPIA MÃ³dulo 5: IntegraciÃ³n Spark + Kafka

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![PySpark](https://img.shields.io/badge/PySpark-3.5.0-orange.svg)](https://spark.apache.org/)
[![Kafka](https://img.shields.io/badge/Kafka-2.0+-red.svg)](https://kafka.apache.org/)

Este repositorio contiene los entregables del MÃ³dulo 5 del Diplomado en IngenierÃ­a y AnÃ¡lisis de Datos (DEPIA), enfocado en la integraciÃ³n de Apache Spark con Apache Kafka para procesamiento de datos en tiempo real y batch.

## ğŸ“‹ Tabla de Contenidos

- [DescripciÃ³n del Proyecto](#descripciÃ³n-del-proyecto)
- [Arquitectura](#arquitectura)
- [Estructura del Proyecto](#estructura-del-proyecto)
- [Requisitos Previos](#requisitos-previos)
- [InstalaciÃ³n](#instalaciÃ³n)
- [Uso](#uso)
- [Ejemplos de EjecuciÃ³n](#ejemplos-de-ejecuciÃ³n)
- [Componentes](#componentes)
- [Resultados Esperados](#resultados-esperados)

## ğŸ“– DescripciÃ³n del Proyecto

Este proyecto demuestra la integraciÃ³n entre Apache Spark y Apache Kafka para el procesamiento de flujos de datos en tiempo real. Se implementan tres componentes principales:

1. **Productor de Kafka**: Genera datos simulados de sensores (temperatura, humedad, presiÃ³n)
2. **Consumidor con Spark Streaming**: Procesa datos en tiempo real con agregaciones por ventanas de tiempo
3. **Procesador Batch**: Genera reportes estadÃ­sticos sobre datos histÃ³ricos

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kafka Producer â”‚  (Genera datos de sensores)
â”‚  kafka_producer â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kafka Topic    â”‚  (input_topic)
â”‚  Input Stream   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                      â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Spark Streaming  â”‚   â”‚  Spark Batch      â”‚   â”‚ Otros Consumers  â”‚
â”‚ (Real-time)      â”‚   â”‚  (Historical)     â”‚   â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚
         â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kafka Topic    â”‚   â”‚  Archivos CSV/    â”‚
â”‚  (output_topic) â”‚   â”‚  Parquet Reports  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flujo de Datos

1. **Ingesta**: El productor genera eventos de sensores y los publica en `input_topic`
2. **Procesamiento Streaming**: Spark Structured Streaming consume los datos, aplica agregaciones por ventanas de tiempo (5 minutos) y calcula estadÃ­sticas promedio
3. **Procesamiento Batch**: Lee un lote completo de datos histÃ³ricos y genera reportes estadÃ­sticos por sensor
4. **Salida**: Resultados se escriben en consola, tÃ³picos de Kafka o archivos

## ğŸ“ Estructura del Proyecto

```
DEPIA_Spark_Kafka/
â”‚
â”œâ”€â”€ src/                          # CÃ³digo fuente
â”‚   â”œâ”€â”€ kafka_producer.py         # Productor de mensajes Kafka
â”‚   â”œâ”€â”€ spark_streaming_consumer.py  # Consumidor streaming con Spark
â”‚   â””â”€â”€ spark_batch_processor.py  # Procesador batch con Spark
â”‚
â”œâ”€â”€ config/                       # Archivos de configuraciÃ³n
â”‚   â””â”€â”€ config.env               # Variables de entorno
â”‚
â”œâ”€â”€ data/                        # Directorio para datos y reportes
â”‚   â””â”€â”€ .gitkeep
â”‚
â”œâ”€â”€ logs/                        # Logs de ejecuciÃ³n
â”‚   â””â”€â”€ .gitkeep
â”‚
â”œâ”€â”€ requirements.txt             # Dependencias Python
â”œâ”€â”€ .gitignore                  # Archivos a ignorar en Git
â””â”€â”€ README.md                   # Este archivo
```

## ğŸ”§ Requisitos Previos

### Software Necesario

- **Python 3.8+**
- **Java 8 o 11** (requerido por Spark)
- **Apache Kafka 2.0+** (servidor corriendo localmente o remoto)
- **Apache Spark 3.5.0+** (se instala con PySpark)

### VerificaciÃ³n de Java

```bash
java -version
```

### InstalaciÃ³n de Kafka (opcional si no estÃ¡ instalado)

Para sistemas Unix/Linux/Mac:

```bash
# Descargar Kafka
wget https://downloads.apache.org/kafka/3.6.0/kafka_2.13-3.6.0.tgz
tar -xzf kafka_2.13-3.6.0.tgz
cd kafka_2.13-3.6.0

# Iniciar Zookeeper
bin/zookeeper-server-start.sh config/zookeeper.properties &

# Iniciar Kafka Server
bin/kafka-server-start.sh config/server.properties &
```

## ğŸ“¦ InstalaciÃ³n

### 1. Clonar el repositorio

```bash
git clone https://github.com/corvaland/DEPIA_Spark_Kafka.git
cd DEPIA_Spark_Kafka
```

### 2. Crear entorno virtual (recomendado)

```bash
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

### 3. Instalar dependencias

```bash
pip install -r requirements.txt
```

## ğŸš€ Uso

### PreparaciÃ³n: Crear tÃ³picos de Kafka

Antes de ejecutar los scripts, crear los tÃ³picos necesarios:

```bash
# TÃ³pico de entrada
kafka-topics.sh --create --topic input_topic \
    --bootstrap-server localhost:9092 \
    --partitions 3 \
    --replication-factor 1

# TÃ³pico de salida (opcional)
kafka-topics.sh --create --topic output_topic \
    --bootstrap-server localhost:9092 \
    --partitions 3 \
    --replication-factor 1

# Verificar tÃ³picos creados
kafka-topics.sh --list --bootstrap-server localhost:9092
```

### EjecuciÃ³n de Componentes

#### 1. Productor de Kafka

Genera datos simulados de sensores:

```bash
python src/kafka_producer.py
```

Este script:
- Genera 100 mensajes (configurable)
- EnvÃ­a un mensaje cada 2 segundos
- Simula datos de 10 sensores con temperatura, humedad y presiÃ³n

#### 2. Consumidor con Spark Streaming

Procesa datos en tiempo real:

```bash
python src/spark_streaming_consumer.py
```

Este script:
- Lee datos desde `input_topic`
- Aplica agregaciones por ventana de 5 minutos
- Calcula promedios de temperatura, humedad y presiÃ³n por sensor
- Muestra resultados en consola

#### 3. Procesador Batch

Genera reportes estadÃ­sticos:

```bash
python src/spark_batch_processor.py
```

Este script:
- Lee todos los datos disponibles en el tÃ³pico
- Genera estadÃ­sticas (promedio, mÃ­nimo, mÃ¡ximo) por sensor
- Guarda el reporte en formato CSV en `data/sensor_report/`

## ğŸ“Š Ejemplos de EjecuciÃ³n

### Ejemplo 1: Pipeline Completo

Terminal 1 - Productor:
```bash
$ python src/kafka_producer.py
Iniciando productor de datos para el tÃ³pico 'input_topic'...
Presiona Ctrl+C para detener
Mensaje enviado a input_topic particiÃ³n 0 offset 0
Mensaje 1: {'timestamp': '2024-01-19T10:30:45.123456', 'sensor_id': 'sensor_3', ...}
```

Terminal 2 - Streaming:
```bash
$ python src/spark_streaming_consumer.py
Iniciando procesamiento con Spark Structured Streaming...
Leyendo datos desde el tÃ³pico 'input_topic'...
Procesamiento iniciado. Esperando datos...

-------------------------------------------
Batch: 1
-------------------------------------------
+------------------------------------------+----------+------------------+------------------+------------------+-------------+
|window                                    |sensor_id |avg_temperature   |avg_humidity      |avg_pressure      |num_readings |
+------------------------------------------+----------+------------------+------------------+------------------+-------------+
|{2024-01-19 10:30:00, 2024-01-19 10:35:00}|sensor_1  |25.34             |65.22             |1012.45           |15           |
|{2024-01-19 10:30:00, 2024-01-19 10:35:00}|sensor_2  |23.45             |70.15             |1015.20           |18           |
```

### Ejemplo 2: Reporte Batch

```bash
$ python src/spark_batch_processor.py
=== Procesamiento Batch con Spark ===
Leyendo datos desde Kafka: input_topic

Total de registros leÃ­dos: 100

--- Generando reporte por sensor ---
+----------+--------------+-----------------+-----------------+-----------------+
|sensor_id |total_readings|avg_temperature  |min_temperature  |max_temperature  |...
+----------+--------------+-----------------+-----------------+-----------------+
|sensor_1  |10            |24.56            |18.30            |32.10            |...
|sensor_2  |8             |25.12            |19.45            |31.20            |...

âœ“ Procesamiento completado exitosamente
Reporte guardado en: data/sensor_report
```

## ğŸ” Componentes

### 1. kafka_producer.py

**Clase Principal**: `DataProducer`

**MÃ©todos clave**:
- `generate_sample_data()`: Genera datos aleatorios de sensores
- `send_data(data)`: EnvÃ­a un mensaje a Kafka
- `produce_continuous(interval, num_messages)`: ProducciÃ³n continua de mensajes

**ConfiguraciÃ³n**:
- Servidor Kafka: `localhost:9092`
- TÃ³pico: `input_topic`
- Intervalo: 2 segundos

### 2. spark_streaming_consumer.py

**Clase Principal**: `SparkKafkaProcessor`

**MÃ©todos clave**:
- `read_from_kafka(topic)`: Lee stream desde Kafka
- `process_data(df)`: Aplica transformaciones y agregaciones
- `write_to_console(df)`: Escribe resultados en consola
- `write_to_kafka(df, topic)`: Escribe resultados a Kafka

**CaracterÃ­sticas**:
- Ventana de tiempo: 5 minutos
- Watermark: 10 minutos
- Modo de salida: Update

### 3. spark_batch_processor.py

**Clase Principal**: `SparkBatchProcessor`

**MÃ©todos clave**:
- `read_batch_from_kafka(kafka_servers, topic)`: Lectura batch
- `generate_report(df)`: Genera estadÃ­sticas agregadas
- `save_report(df, output_path)`: Guarda reporte en disco

**Formatos de salida soportados**:
- CSV
- Parquet
- JSON

## ğŸ“ˆ Resultados Esperados

### MÃ©tricas Calculadas

#### Streaming (por ventana de 5 minutos):
- Temperatura promedio por sensor
- Humedad promedio por sensor
- PresiÃ³n promedio por sensor
- NÃºmero de lecturas por sensor

#### Batch (histÃ³rico completo):
- Total de lecturas por sensor
- Temperatura: promedio, mÃ­nima, mÃ¡xima
- Humedad: promedio, mÃ­nima, mÃ¡xima
- PresiÃ³n: promedio, mÃ­nima, mÃ¡xima

### Archivos Generados

- `data/sensor_report/*.csv`: Reporte estadÃ­stico en CSV
- Checkpoints en `/tmp/spark_checkpoint_*`: Estados de Spark Streaming

## ğŸ”’ ConfiguraciÃ³n Avanzada

### Ajustar parÃ¡metros en config.env

```bash
# Servidores Kafka
KAFKA_BOOTSTRAP_SERVERS=localhost:9092

# TÃ³picos
KAFKA_INPUT_TOPIC=input_topic
KAFKA_OUTPUT_TOPIC=output_topic

# Spark
SPARK_APP_NAME=DEPIA_Spark_Kafka_Integration
CHECKPOINT_LOCATION=/tmp/spark_checkpoint

# Procesamiento
BATCH_DURATION=10
```

### Escalabilidad

Para ambientes productivos, considerar:

1. **MÃºltiples particiones**: Aumentar el nÃºmero de particiones en Kafka
2. **Paralelismo de Spark**: Ajustar `spark.default.parallelism`
3. **Memoria**: Configurar `spark.executor.memory` y `spark.driver.memory`
4. **Checkpointing**: Usar almacenamiento distribuido (HDFS, S3) para checkpoints

## ğŸ› ï¸ SoluciÃ³n de Problemas

### Error: "Kafka server not found"
- Verificar que Kafka estÃ© corriendo: `netstat -an | grep 9092`
- Revisar configuraciÃ³n de `bootstrap.servers`

### Error: "Java not found"
- Instalar Java: `sudo apt-get install openjdk-11-jdk`
- Configurar JAVA_HOME

### Error: "Checkpoint directory already exists"
- Eliminar checkpoints antiguos: `rm -rf /tmp/spark_checkpoint_*`

## ğŸ“š Referencias

- [Apache Spark Structured Streaming + Kafka](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)
- [Kafka Python Client](https://kafka-python.readthedocs.io/)
- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)

## ğŸ‘¥ Autor

Proyecto desarrollado como entregable del MÃ³dulo 5 de DEPIA (Diplomado en IngenierÃ­a y AnÃ¡lisis de Datos)

## ğŸ“„ Licencia

Este proyecto es material educativo para el curso DEPIA.
